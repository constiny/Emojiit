{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "--------------\n",
    "## first check the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18866900 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many lines\n",
    "! wc -l data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209430000 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many words\n",
    "! wc -w data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.100392751326398"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On average the sentense are have 11 words\n",
    "209430000/18866900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test - validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15093520.0, 1886690.0, 3773380.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on 80-10-10\n",
    "18866900 * 0.8, 18866900 * 0.1, 18866900 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -15093520 data/emojitweets-01-04-2018.txt >>emoji_train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: error writing 'standard output': Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! (tail -3773380 data/emojitweets-01-04-2018.txt | head -1886690) >> emoji_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -1886690 data/emojitweets-01-04-2018.txt >> emoji_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate small_sample\n",
    "! head -1509352 emoji_train.txt >>emoji_ss.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  \n",
    "# sample = open(\"emoji_1M.txt\", \"r\") \n",
    "sample = open(\"emoji_train.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sample = open(\"emoji_ss.txt\", \"r\") \n",
    "s_ss = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f_ss = s_ss.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Tokenize and stop words\n",
    "\n",
    "# 1. Create a set of documents.\n",
    "documents = [' '.join(article['content']).lower() for article in coll.find()]\n",
    "\n",
    "# 2. Create a set of tokenized documents.\n",
    "docs = [word_tokenize(content) for content in documents]\n",
    "\n",
    "# 3. Strip out stop words from each tokenized document.\n",
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if word not in stop] for words in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_set = set(split_count(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_list = list(emoji_set)\n",
    "with open(\"emoji_list.csv\", 'w', newline='') as myfile:\n",
    "     w = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     w.writerow(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad',\n",
       " 'arriving',\n",
       " 'for',\n",
       " 'game',\n",
       " '2',\n",
       " 'üöÄ',\n",
       " 'dude',\n",
       " 'is',\n",
       " 'like',\n",
       " '5',\n",
       " '‚Äô',\n",
       " '8',\n",
       " '140',\n",
       " 'pounds',\n",
       " 'his',\n",
       " 'dick',\n",
       " 'was',\n",
       " 'long',\n",
       " 'and',\n",
       " 'strong',\n",
       " '(',\n",
       " 'always',\n",
       " 'the',\n",
       " 'little',\n",
       " 'dudes',\n",
       " 'carrying',\n",
       " 'the',\n",
       " 'üçÜ',\n",
       " ')',\n",
       " '\\U0001f92aüôÉ',\n",
       " 'followersüëá',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'breatiuhw',\n",
       " 'üíÄüíÄüíÄ',\n",
       " '2Ô∏è‚É£4Ô∏è‚É£',\n",
       " 'hours',\n",
       " \"'til\",\n",
       " 'our',\n",
       " 'schedule',\n",
       " 'drops',\n",
       " '!']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "import pickle\n",
    "with open('cbow.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 1606239500,\n",
       " 'vectors': 1284991600,\n",
       " 'syn1neg': 1284991600,\n",
       " 'total': 4176222700}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('motorcycle', 0.7577804327011108),\n",
       " ('yacht', 0.6977012157440186),\n",
       " ('üö≤', 0.6723682880401611),\n",
       " ('bicycle', 0.6688296794891357),\n",
       " ('bikes', 0.6612945199012756),\n",
       " ('horse', 0.6549836993217468),\n",
       " ('mattress', 0.6493205428123474),\n",
       " ('buggy', 0.6399540901184082),\n",
       " ('car', 0.6371113061904907),\n",
       " ('engine', 0.6342647075653076),\n",
       " ('lawn', 0.6337512135505676),\n",
       " ('truck', 0.6288256645202637),\n",
       " ('kayak', 0.6270779371261597),\n",
       " ('motorbike', 0.6254085898399353),\n",
       " ('jet', 0.6219853758811951),\n",
       " ('ferrari', 0.6187005639076233),\n",
       " ('vehicle', 0.6166753768920898),\n",
       " ('jeep', 0.6139025688171387),\n",
       " ('horseback', 0.6051432490348816),\n",
       " ('tractor', 0.6044219732284546),\n",
       " ('paddle', 0.603623628616333),\n",
       " ('tesla', 0.6030915975570679),\n",
       " ('üöò', 0.6017401218414307),\n",
       " ('route', 0.5992273688316345),\n",
       " ('yeti', 0.5969487428665161),\n",
       " ('towel', 0.5940559506416321),\n",
       " ('skateboard', 0.5918545126914978),\n",
       " ('honda', 0.5908911824226379),\n",
       " ('rear', 0.5888210535049438),\n",
       " ('highway', 0.5880736112594604),\n",
       " ('rug', 0.5873566269874573),\n",
       " ('hike', 0.5864423513412476),\n",
       " ('bus', 0.586387574672699),\n",
       " ('rope', 0.585216224193573),\n",
       " ('üèç', 0.5827182531356812),\n",
       " ('yard', 0.5815604329109192),\n",
       " ('trampoline', 0.5795732736587524),\n",
       " ('wraith', 0.5790944695472717),\n",
       " ('cadillac', 0.575817883014679),\n",
       " ('helicopter', 0.5757952332496643),\n",
       " ('scooter', 0.5739297866821289),\n",
       " ('motor', 0.5728486180305481),\n",
       " ('wall', 0.5726180672645569),\n",
       " ('quad', 0.5711467862129211),\n",
       " ('lexus', 0.5690762996673584),\n",
       " ('cab', 0.5686187744140625),\n",
       " ('üöó', 0.5683943033218384),\n",
       " ('ceiling', 0.5671595335006714),\n",
       " ('ride', 0.563444972038269),\n",
       " ('cruise', 0.5624530911445618)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"bike\"\n",
    "sim = model1.most_similar(word,topn=50)\n",
    "for i in sim:\n",
    "    if i in emoji_list:\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = list(emoji_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_prodictor(word):\n",
    "    sim = model1.most_similar(word,topn=50)\n",
    "    for i in sim:\n",
    "        if i[0] in emoji_list:\n",
    "            return i[0]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's\n",
      "go\n",
      "grocery\n",
      "today\n",
      "ü§∑üèΩ‚Äç‚ôÄÔ∏è\n",
      "get\n",
      "üòÇ\n",
      "üö≤\n",
      "related\n",
      "stuff\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery today to get some bike related stuff\"\n",
    "for w in s1.lower().split(\" \"):\n",
    "    print(emoji_prodictor(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor(\"bike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
