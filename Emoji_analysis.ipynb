{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "--------------\n",
    "## first check the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18866900 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many lines\n",
    "! wc -l data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209430000 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many words\n",
    "! wc -w data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.100392751326398"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On average the sentense are have 11 words\n",
    "209430000/18866900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test - validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15093520.0, 1886690.0, 3773380.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on 80-10-10\n",
    "18866900 * 0.8, 18866900 * 0.1, 18866900 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -15093520 data/emojitweets-01-04-2018.txt >>emoji_train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: error writing 'standard output': Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! (tail -3773380 data/emojitweets-01-04-2018.txt | head -1886690) >> emoji_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -1886690 data/emojitweets-01-04-2018.txt >> emoji_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate small_sample\n",
    "! head -1509352 data/emoji_train.txt >> data/emoji_ss.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"emoji_1M.txt\", \"r\") \n",
    "sample = open(\"emoji_ss.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Squad arriving for Game 2 ğŸš€\\nDude is like 5â€™8 140 pounds his dick was long and strong(always the little dudes carrying the ğŸ†) \\U0001f92ağŸ™ƒ\\nFOLLOWERSğŸ‘‡\\nI CANT BREATIUHW ğŸ’€ğŸ’€ğŸ’€\\n2ï¸âƒ£4ï¸âƒ£ hours 'til our schedule drops!\\nNEW || Zach &amp; Jack at Limelight tonight! (April 17) Â©ï¸nvmbesson\\nIâ€™m live at Milk Boy studio the home of  making some hits ğŸ’ªğŸ¾\\nI am SO scared of birdsğŸ¤§\\nThatâ€™s me ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\\nMy heart is so full rn ğŸ’–ğŸ’–\\nThis is one of my favorite songs to sing in this episode â¤ï¸\\nTook my goat to get groomed for the first tim\""
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"emoji_1M.txt\", \"r\") \n",
    "sample = open(\"emoji_ss.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "# f = s.replace(\"\\n\", \" \") \n",
    "f = s\n",
    "\n",
    "\n",
    "# replace all \n",
    "notes = [\",\", \".\", \"/'\", \"(\" ,\")\", \"'\", '/\"', \":\", \";\", \"/\", \"(\", \")\", \"â€œ\", \"â€\", \"-\", \"+\", \"#\", \"â€¦\", \"!\"]\n",
    "for note in notes:\n",
    "    f = f.replace(note, \"\")\n",
    "  \n",
    "data = [] \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to helper\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"../emoji_100k.txt\", \"r\") \n",
    "# s_ss = sample.read() \n",
    "  \n",
    "# # Replaces escape character with space\n",
    "# f_ss = s_ss.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "# # replace all \n",
    "# signs = [\",\", \".\", \"(\" ,\")\", \"'\", '/\"', \":\", \";\", \"/\", \"(\", \")\"]\n",
    "# for sign in signs:\n",
    "#     f_ss = f_ss.replace(sign, \"\")\n",
    "  \n",
    "# data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = f_ss\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and snowball stemmer\n",
    "out = [[word for word in words if word not in stop] for words in data]\n",
    "snowball = SnowballStemmer('english')\n",
    "docs_snowball = [[snowball.stem(word) for word in words] for words in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79777"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_snowball[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save emoji list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_list = list(emoji_set)\n",
    "# with open(\"emoji_list.csv\", 'w', newline='') as myfile:\n",
    "#      w = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#      w.writerow(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"emoji_list.csv\", 'r', newline='') as myfile:\n",
    "#      emoji_list = csv.reader(myfile, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(docs_snowball, \n",
    "                                min_count = 10,  \n",
    "                                size = 300, \n",
    "                                window = 16,\n",
    "                                workers= 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "import pickle\n",
    "with open('cbow_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 19961500,\n",
       " 'vectors': 31938400,\n",
       " 'syn1neg': 31938400,\n",
       " 'total': 83838300}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/emoji_val.2csv\",index_col=0)\n",
    "emoji_list = val_df[\"emoji\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to helper\n",
    "def emoji_prodictor(word, model):\n",
    "    sim = model.most_similar(word,topn=75)\n",
    "    for i in sim:\n",
    "        if i[0] in emoji_list:\n",
    "            return i[0]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'go', 'groceri', 'buy', 'appl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move to helper\n",
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out\n",
    "    \n",
    "    \n",
    "s1 = \"let's go grocery and buy some apple\"\n",
    "word_pipeline(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤·â€â™€ï¸\n",
      "ğŸ¤¦ğŸ¾â€â™€ï¸\n",
      "ğŸ›’\n",
      "ğŸ’¸\n",
      "ğŸ‘ğŸ½\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery and buy some apple\"\n",
    "for w in word_pipeline(s1):\n",
    "    print(emoji_prodictor(w, model1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ‘ŒğŸ¼'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor(\"today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skip-gram model \n",
    "model2 = gensim.models.Word2Vec(docs_snowball, \n",
    "                                min_count = 10,  \n",
    "                                size = 300, \n",
    "                                window = 16,\n",
    "                                sg = 1,\n",
    "                                workers=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save skip-gram model\n",
    "import pickle\n",
    "with open('sg_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜‚\n",
      "ğŸ˜‚\n",
      "ğŸ˜€\n",
      "buy\n",
      "ğŸ\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery and buy some apples\"\n",
    "for w in word_pipeline(s1):\n",
    "    print(emoji_prodictor(w, model2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nowğŸ®', 0.7763185501098633),\n",
       " ('puzzl', 0.7215666770935059),\n",
       " ('now~ğŸ®', 0.6516302227973938),\n",
       " ('onğŸ’¡', 0.6338413953781128),\n",
       " ('app', 0.6186978816986084),\n",
       " ('allğŸ˜†', 0.5676931738853455),\n",
       " ('â—ï¸freeâ—ï¸', 0.5613607168197632),\n",
       " ('nowâ€¼ï¸dino', 0.5496898293495178),\n",
       " ('microphon', 0.5196519494056702),\n",
       " ('goâ€¼ï¸', 0.5183366537094116),\n",
       " ('ğŸ°', 0.5135799646377563),\n",
       " ('â€¼ï¸brand', 0.508499026298523),\n",
       " ('store', 0.5015548467636108),\n",
       " ('iiv', 0.47544899582862854),\n",
       " ('availableâ€¼ï¸', 0.4744521677494049),\n",
       " ('ğŸ™‹', 0.45962515473365784),\n",
       " ('ğŸ˜âœ¨', 0.4565337300300598),\n",
       " ('star', 0.4419368803501129),\n",
       " ('apk', 0.4328591525554657),\n",
       " ('dailymot', 0.4285903573036194)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"googl\",topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/emoji_val.2csv\",index_col=0)\n",
    "emoji_set = val_df[\"emoji\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"full_desc\"] = val_df.full_desc.apply(lambda x: x.strip('][').split(', '))\n",
    "val_df[\"desc2\"] = val_df.desc2.apply(lambda x: x.strip('][').split(', ') if isinstance(x,str) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_df.index:\n",
    "    for j in range(len(val_df.full_desc[i])):\n",
    "#         print(i)\n",
    "        val_df.full_desc[i][j] = val_df.full_desc[i][j].strip(\"/'\")\n",
    "    if val_df.desc2[i]:\n",
    "        for k in range(len(val_df.desc2[i])):\n",
    "    #         print(i)\n",
    "            val_df.desc2[i][k] = val_df.desc2[i][k].strip(\"/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grin'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"desc2\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation set\n",
    "val_dict = {}\n",
    "lst = []\n",
    "for i in val_df.index.tolist():\n",
    "    lst.extend(val_df.full_desc[i])\n",
    "    lst.extend(val_df.desc2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(set(lst))\n",
    "for val in vals:\n",
    "    val_dict[val] = []\n",
    "    for index in val_df.index:\n",
    "        \n",
    "        if val in val_df.full_desc[index]:\n",
    "            val_dict[val].append(val_df.emoji[index])\n",
    "        elif val in val_df.desc2[index]:\n",
    "            val_dict[val].append(val_df.emoji[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with open('val_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update validation set\n",
    "# val_dict2 = dict();\n",
    "# for key, value in val_dict.items():\n",
    "#     if re.split('[- ]', key) == 1:\n",
    "#         a = key.strip('[():â€œâ€]')\n",
    "#         if a not in val_dict2:\n",
    "#             val_dict2[a] = value\n",
    "#         else:\n",
    "#             val_dict2[a].append(value)\n",
    "#     else:\n",
    "#         for i in re.split('[- ]', key):\n",
    "#             i2 = i.strip('[():â€œâ€]')\n",
    "#             if i2 not in val_dict2:\n",
    "#                 val_dict2[i2] = value\n",
    "#             else:\n",
    "#                 val_dict2[i2].append(value)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('val_dict2.pickle', 'wb') as handle:\n",
    "#     pickle.dump(val_dict2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_dict.pickle', 'rb') as handle:\n",
    "    val_dict = pickle.load(handle)\n",
    "\n",
    "# print val_dict == bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_score(val_dict, model):\n",
    "    total_words = 0\n",
    "    total_predicts = 0\n",
    "    correct = 0\n",
    "    words = []\n",
    "    word_vectors = model.wv\n",
    "    for word in val_dict:\n",
    "#         print(word)\n",
    "        if word_pipeline(word):\n",
    "            w = word_pipeline(word)[0]\n",
    "            words.append(w)\n",
    "            if w in word_vectors:\n",
    "                total_words += 1\n",
    "                if emoji_prodictor(w, model) in val_dict[word]:\n",
    "                    correct += 1\n",
    "                    total_predicts += 1\n",
    "                elif emoji_prodictor(w, model) in emoji_set:\n",
    "                    total_predicts += 1\n",
    "    return correct/total_words, correct/total_predicts, words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = word2vec_score(val_dict, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = word2vec_score(val_dict, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.16936488169364883, 0.19470293486041518),\n",
       " (0.22478206724782068, 0.2565742714996446))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 300, window = 16 and new notes\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.16084788029925187, 0.19068736141906872),\n",
       " (0.21571072319201995, 0.25181950509461426))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 300, window = 16\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.15149625935162095, 0.17711370262390672),\n",
       " (0.21571072319201995, 0.24162011173184358))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 300, window = 20\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.1589775561097257, 0.19480519480519481),\n",
       " (0.18952618453865336, 0.22943396226415094))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 150, window = 10\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.17206982543640897, 0.21052631578947367),\n",
       " (0.2013715710723192, 0.2492283950617284))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 300, window = 10\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.16521197007481297, 0.1990984222389181),\n",
       " (0.20199501246882792, 0.2577565632458234))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 500, window = 10\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.16390532544378697, 0.22124600638977635),\n",
       " (0.17810650887573964, 0.2872137404580153))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 5,  size = 500, window = 8\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.16209476309226933, 0.19667170953101362),\n",
       " (0.19887780548628428, 0.2570507655116841))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 500, window = 8\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.14526184538653367, 0.19082719082719082),\n",
       " (0.1502493765586035, 0.21929026387625114))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for SS WITH min_count = 10,  size = 500, window = 3\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.14964788732394366, 0.18695014662756598),\n",
       " (0.15307262569832403, 0.23539518900343642))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # score for CBOW and skipgram for small sample with SnowballStemmer\n",
    "# score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘', 0.5708750486373901),\n",
       " ('ğŸ˜‰', 0.5507802963256836),\n",
       " ('ğŸ˜', 0.5429814457893372),\n",
       " ('â­ï¸i', 0.5380228757858276),\n",
       " ('awesom', 0.5341282486915588),\n",
       " ('ğŸ˜…', 0.5143481492996216),\n",
       " ('ğŸ¤—', 0.5049486756324768),\n",
       " ('ğŸ˜', 0.503909707069397),\n",
       " ('ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', 0.4923427402973175),\n",
       " ('ğŸ˜˜', 0.4906120002269745),\n",
       " ('wonder', 0.48665982484817505),\n",
       " ('goan', 0.4851222038269043),\n",
       " ('jeanett', 0.47811296582221985),\n",
       " ('skyrim', 0.4774525463581085),\n",
       " ('ğŸ™€ğŸ™€ğŸ™€', 0.47732964158058167),\n",
       " ('ğŸ˜Š', 0.47706007957458496),\n",
       " ('ğŸ‘ğŸ‘Š', 0.4713195264339447),\n",
       " ('carsss', 0.47071343660354614),\n",
       " ('ğŸ˜thank', 0.4696727693080902),\n",
       " ('ğŸ’ğŸŒºğŸ’•', 0.4684681296348572)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('ğŸ˜€',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor('happi', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ğŸ˜€',\n",
       " 'ğŸ˜€',\n",
       " 'ğŸ˜ƒ',\n",
       " 'ğŸ˜ƒ',\n",
       " 'ğŸ˜„',\n",
       " 'ğŸ˜„',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜†',\n",
       " 'ğŸ˜†',\n",
       " 'ğŸ˜…',\n",
       " 'ğŸ˜…',\n",
       " 'ğŸ¤£',\n",
       " 'ğŸ¤£',\n",
       " 'ğŸ˜‚',\n",
       " 'ğŸ˜‚',\n",
       " 'ğŸ™‚',\n",
       " 'ğŸ™‚',\n",
       " 'ğŸ™ƒ',\n",
       " 'ğŸ™ƒ',\n",
       " 'ğŸ˜‰',\n",
       " 'ğŸ˜‰',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜‡',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜˜',\n",
       " 'ğŸ˜˜',\n",
       " 'ğŸ˜—',\n",
       " 'â˜ºï¸',\n",
       " 'â˜ºï¸',\n",
       " 'ğŸ˜š',\n",
       " 'ğŸ˜š',\n",
       " 'ğŸ˜™',\n",
       " 'ğŸ˜‹',\n",
       " 'ğŸ˜‹',\n",
       " 'ğŸ˜›',\n",
       " 'ğŸ˜œ',\n",
       " 'ğŸ˜œ',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ¤‘',\n",
       " 'ğŸ¤‘',\n",
       " 'ğŸ¤—',\n",
       " 'ğŸ¤—',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜Œ',\n",
       " 'ğŸ˜Œ',\n",
       " 'ğŸ¤¤',\n",
       " 'ğŸ¤ ',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ¤“',\n",
       " 'ğŸ¤“',\n",
       " 'ğŸ˜®',\n",
       " 'ğŸ˜¤',\n",
       " 'ğŸ˜¤',\n",
       " 'ğŸ¤¡',\n",
       " 'ğŸ¤¡',\n",
       " 'ğŸ™‹',\n",
       " 'ğŸ™‹ğŸ»',\n",
       " [...]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dict['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = \"under\"\n",
    "[word for word in sentense.lower().split(\" \") if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pipeline(sentense)model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ˜€'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor('us', model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸšŒ'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor('nyc', model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sherlock_homes.txt', 'r') as file:\n",
    "    text = file.read().lower()\n",
    "print('text length', len(text))\n",
    "chars = sorted(list(set(text))) # getting all unique chars\n",
    "print('total chars: ', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(500, 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
