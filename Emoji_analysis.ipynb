{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "--------------\n",
    "## first check the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18866900 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many lines\n",
    "! wc -l data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209430000 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many words\n",
    "! wc -w data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.100392751326398"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On average the sentense are have 11 words\n",
    "209430000/18866900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test - validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15093520.0, 1886690.0, 3773380.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on 80-10-10\n",
    "18866900 * 0.8, 18866900 * 0.1, 18866900 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -15093520 data/emojitweets-01-04-2018.txt >>emoji_train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: error writing 'standard output': Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! (tail -3773380 data/emojitweets-01-04-2018.txt | head -1886690) >> emoji_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -1886690 data/emojitweets-01-04-2018.txt >> emoji_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate small_sample\n",
    "! head -1509352 data/emoji_train.txt >> data/emoji_ss.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"emoji_1M.txt\", \"r\") \n",
    "sample = open(\"emoji_train.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "  \n",
    "data = [] \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sample = open(\"../emoji_100k.txt\", \"r\") \n",
    "s_ss = sample.read() \n",
    "  \n",
    "# Replaces escape character with space\n",
    "f_ss = s_ss.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "# replace all \n",
    "signs = [\",\", \".\", \"(\" ,\")\", \"'\", '/\"', \":\", \";\", \"/\", \"(\", \")\"]\n",
    "for sign in signs:\n",
    "    f_ss = f_ss.replace(sign, \"\")\n",
    "  \n",
    "data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f_ss\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and snowball stemmer\n",
    "out = [[word for word in words if word not in stop] for words in data]\n",
    "snowball = SnowballStemmer('english')\n",
    "docs_snowball = [[snowball.stem(word) for word in words] for words in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad',\n",
       " 'arriv',\n",
       " 'game',\n",
       " '2',\n",
       " 'üöÄ',\n",
       " 'dude',\n",
       " 'like',\n",
       " '5',\n",
       " '‚Äô',\n",
       " '8',\n",
       " '140',\n",
       " 'pound',\n",
       " 'dick',\n",
       " 'long',\n",
       " 'strong',\n",
       " 'alway',\n",
       " 'littl',\n",
       " 'dude',\n",
       " 'carri',\n",
       " 'üçÜ',\n",
       " '\\U0001f92aüôÉ',\n",
       " 'followersüëá',\n",
       " 'cant',\n",
       " 'breatiuhw',\n",
       " 'üíÄüíÄüíÄ',\n",
       " '2Ô∏è‚É£4Ô∏è‚É£',\n",
       " 'hour',\n",
       " 'til',\n",
       " 'schedul',\n",
       " 'drop',\n",
       " '!']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_snowball[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save emoji list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_list = list(emoji_set)\n",
    "# with open(\"emoji_list.csv\", 'w', newline='') as myfile:\n",
    "#      w = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#      w.writerow(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'emoji_list.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a119b8888afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emoji_list.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0memoji_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emoji_list.csv'"
     ]
    }
   ],
   "source": [
    "with open(\"emoji_list.csv\", 'r', newline='') as myfile:\n",
    "     emoji_list = csv.reader(myfile, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-323185ebf54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create CBOW model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model1 = gensim.models.Word2Vec(data, min_count = 1,  \n\u001b[0;32m----> 3\u001b[0;31m                               size = 100, window = 5) \n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    942\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1874\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1891\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mseeded_vector\u001b[0;34m(self, seed_string, vector_size)\u001b[0m\n\u001b[1;32m   1881\u001b[0m         \u001b[0;34m\"\"\"Get a random vector (but deterministic by seed_string).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0;31m# Note: built-in hash() may vary by Python version or even (in Py3.x) per launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0monce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xffffffff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "import pickle\n",
    "with open('cbow_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 1606239500,\n",
       " 'vectors': 1284991600,\n",
       " 'syn1neg': 1284991600,\n",
       " 'total': 4176222700}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = list(emoji_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_prodictor(word):\n",
    "    sim = model1.most_similar(word,topn=50)\n",
    "    for i in sim:\n",
    "        if i[0] in emoji_list:\n",
    "            return i[0]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's\n",
      "go\n",
      "grocery\n",
      "today\n",
      "ü§∑üèΩ‚Äç‚ôÄÔ∏è\n",
      "get\n",
      "üòÇ\n",
      "üö≤\n",
      "related\n",
      "stuff\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery today to get some bike related stuff\"\n",
    "for w in s1.lower().split(\" \"):\n",
    "    print(emoji_prodictor(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bike'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor(\"bike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_set = set(split_count(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_set = list(emoji_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model2 = gensim.models.Word2Vec(docs_snowball, \n",
    "                                min_count = 5,  \n",
    "                                size = 100, \n",
    "                                window = 5,\n",
    "                                workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "import pickle\n",
    "with open('cbow_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_prodictor(word):\n",
    "    sim = model2.most_similar(word,topn=20)\n",
    "    for i in sim:\n",
    "        if i[0] in emoji_set:\n",
    "            return i[0]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'go', 'groceri', 'today', 'get', 'bike', 'relat', 'stuff']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out\n",
    "    \n",
    "    \n",
    "s1 = \"let's go grocery today to get some bike related stuff\"\n",
    "word_pipeline(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§∑‚Äç‚ôÄÔ∏è\n",
      "ü§∑üèº‚Äç‚ôÄÔ∏è\n",
      "groceri\n",
      "üò®\n",
      "ü§∑üèæ‚Äç‚ôÄÔ∏è\n",
      "üöò\n",
      "relat\n",
      "stuff\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery today to get some bike related stuff\"\n",
    "for w in word_pipeline(s1):\n",
    "    print(emoji_prodictor(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 0.6086397767066956),\n",
       " ('üöò', 0.5864426493644714),\n",
       " ('wheel', 0.5853496193885803),\n",
       " ('cruis', 0.5828701853752136),\n",
       " ('highway', 0.5649341940879822),\n",
       " ('hors', 0.552920937538147),\n",
       " ('driveway', 0.5436760783195496),\n",
       " ('kitchen', 0.5423159003257751),\n",
       " ('bus', 0.5340845584869385),\n",
       " ('road', 0.5305561423301697),\n",
       " ('hike', 0.5252187848091125),\n",
       " ('crib', 0.52373206615448),\n",
       " ('tabl', 0.5200468301773071),\n",
       " ('edg', 0.5186354517936707),\n",
       " ('stair', 0.517928957939148),\n",
       " ('truck', 0.5167300701141357),\n",
       " ('desert', 0.5161859393119812),\n",
       " ('beach', 0.5118310451507568),\n",
       " ('around', 0.5103741884231567),\n",
       " ('dread', 0.509566068649292)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"bike\",topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/emoji_val.2csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_set = val_df[\"emoji\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"full_desc\"] = val_df.full_desc.apply(lambda x: x.strip('][').split(', '))\n",
    "# val_df[\"full_desc\"] = val_df.full_desc.apply(lambda x: x.strip('][').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_df.index:\n",
    "    for j in range(len(val_df.full_desc[i])):\n",
    "#         print(i)\n",
    "        val_df.full_desc[i][j] = val_df.full_desc[i][j].strip(\"/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation set\n",
    "val_dict = {}\n",
    "lst = []\n",
    "for i in val_df.index.tolist():\n",
    "    lst.extend(val_df.full_desc[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(set(lst))\n",
    "for val in vals:\n",
    "    val_dict[val] = []\n",
    "    for index in val_df.index:\n",
    "        \n",
    "        if val in val_df.full_desc[index]:\n",
    "            val_dict[val].append(val_df.emoji[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_score(val_dict, model):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    words = []\n",
    "    word_vectors = model.wv\n",
    "    for word in val_dict:\n",
    "#         print(word)\n",
    "        if word_pipeline(word):\n",
    "            w = word_pipeline(word)[0]\n",
    "            words.append(w)\n",
    "            if w in word_vectors:\n",
    "                total += 0\n",
    "                if emoji_prodictor(w) in val_dict[word]:\n",
    "                    correct += 1\n",
    "    return correct, total, words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163,\n",
       " 0,\n",
       " ['reliev',\n",
       "  'five',\n",
       "  'lacross',\n",
       "  'twelv',\n",
       "  'merman',\n",
       "  '3',\n",
       "  'massag',\n",
       "  'penguin',\n",
       "  'luxembourg',\n",
       "  'merperson:',\n",
       "  'speak-no-evil',\n",
       "  'stopwatch',\n",
       "  'eight-thirti',\n",
       "  'crossbon',\n",
       "  'cake',\n",
       "  'drool',\n",
       "  'take',\n",
       "  'frame',\n",
       "  'ecuador',\n",
       "  'ten-thirti',\n",
       "  'atom',\n",
       "  'artist:',\n",
       "  'avocado',\n",
       "  'mammoth',\n",
       "  'handbag',\n",
       "  'bug',\n",
       "  'mantelpiec',\n",
       "  'taco',\n",
       "  'angel:',\n",
       "  'mechan',\n",
       "  'arabia',\n",
       "  'ear',\n",
       "  'floppi',\n",
       "  'spring',\n",
       "  'exchang',\n",
       "  'ghost',\n",
       "  'ok',\n",
       "  'splay',\n",
       "  'lion',\n",
       "  'straw',\n",
       "  'christma',\n",
       "  'motor',\n",
       "  'wood',\n",
       "  'uganda',\n",
       "  'melon',\n",
       "  'hug',\n",
       "  'sake',\n",
       "  'traffic',\n",
       "  'brunei',\n",
       "  'tractor',\n",
       "  'spaghetti',\n",
       "  'volum',\n",
       "  'roll',\n",
       "  'transgend',\n",
       "  '5',\n",
       "  'hear',\n",
       "  'tomato',\n",
       "  'wolf',\n",
       "  'bolivia',\n",
       "  'timer',\n",
       "  '2',\n",
       "  'helicopt',\n",
       "  'desert',\n",
       "  'packag',\n",
       "  'haircut',\n",
       "  'hockey',\n",
       "  'koala',\n",
       "  'sos',\n",
       "  'castl',\n",
       "  'polynesia',\n",
       "  'pr√≠ncipe',\n",
       "  'movi',\n",
       "  'slider',\n",
       "  'thailand',\n",
       "  'needl',\n",
       "  'niue',\n",
       "  'tropic',\n",
       "  'wing',\n",
       "  '‚Äúbargain‚Äù',\n",
       "  'trident',\n",
       "  'curl',\n",
       "  'cupcak',\n",
       "  'claim',\n",
       "  'microphon',\n",
       "  'swimsuit',\n",
       "  'leopard',\n",
       "  'leafi',\n",
       "  'rescu',\n",
       "  'plunger',\n",
       "  'ox',\n",
       "  'two-hump',\n",
       "  'newspap',\n",
       "  'horn',\n",
       "  'ice',\n",
       "  'clap',\n",
       "  'heart:',\n",
       "  'skullcap:',\n",
       "  'lebanon',\n",
       "  'wane',\n",
       "  'envelop',\n",
       "  'bank',\n",
       "  'classic',\n",
       "  'free',\n",
       "  'symbol',\n",
       "  'three-thirti',\n",
       "  'puerto',\n",
       "  'regist',\n",
       "  'chad',\n",
       "  'monster',\n",
       "  'sandal',\n",
       "  'kenya',\n",
       "  'artist',\n",
       "  'airplan',\n",
       "  'soap',\n",
       "  'panda',\n",
       "  'guitar',\n",
       "  'middl',\n",
       "  'turkey',\n",
       "  'prohibit',\n",
       "  'telescop',\n",
       "  'burundi',\n",
       "  'up!',\n",
       "  'weari',\n",
       "  'wed',\n",
       "  'comoro',\n",
       "  'pear',\n",
       "  'park',\n",
       "  'wedg',\n",
       "  'bomb',\n",
       "  'cinema',\n",
       "  'parti',\n",
       "  'leo',\n",
       "  'verd',\n",
       "  'cane',\n",
       "  'rocket',\n",
       "  '2nd',\n",
       "  'cut',\n",
       "  'hut',\n",
       "  'blow',\n",
       "  'servic',\n",
       "  'zambia',\n",
       "  '10',\n",
       "  'salut',\n",
       "  'globe',\n",
       "  'carp',\n",
       "  'chile',\n",
       "  'squar',\n",
       "  'menorah',\n",
       "  'peanut',\n",
       "  'four-thirti',\n",
       "  'food',\n",
       "  'synagogu',\n",
       "  'input',\n",
       "  'popper',\n",
       "  'vibrat',\n",
       "  'gestur',\n",
       "  'receiv',\n",
       "  '‚Äúopen',\n",
       "  'flag',\n",
       "  'perform',\n",
       "  'upside-down',\n",
       "  'mechan',\n",
       "  'coco',\n",
       "  'worship',\n",
       "  'droplet',\n",
       "  'vomit',\n",
       "  'extinguish',\n",
       "  'electr',\n",
       "  'dvd',\n",
       "  'flush',\n",
       "  'monocl',\n",
       "  'row',\n",
       "  'accordion',\n",
       "  'macedonia',\n",
       "  'scorpio',\n",
       "  'guernsey',\n",
       "  'duck',\n",
       "  'isl',\n",
       "  'rais',\n",
       "  'whale',\n",
       "  'flamingo',\n",
       "  'tenni',\n",
       "  'mexico',\n",
       "  'show',\n",
       "  'cri',\n",
       "  'fire',\n",
       "  'thong',\n",
       "  'one-piec',\n",
       "  'saw',\n",
       "  'backhand',\n",
       "  'kick',\n",
       "  'coat',\n",
       "  'head',\n",
       "  'u.s.',\n",
       "  'roller',\n",
       "  'claus:',\n",
       "  'hibiscus',\n",
       "  'bellhop',\n",
       "  'one',\n",
       "  'astonish',\n",
       "  'brazil',\n",
       "  'feed',\n",
       "  'tray',\n",
       "  '6',\n",
       "  'pilot:',\n",
       "  'boar',\n",
       "  'paintbrush',\n",
       "  'caico',\n",
       "  'vatican',\n",
       "  'surf',\n",
       "  'golf',\n",
       "  'ball:',\n",
       "  'calendar',\n",
       "  'down-left',\n",
       "  'blossom',\n",
       "  'orthodox',\n",
       "  'philippin',\n",
       "  'medic',\n",
       "  'call',\n",
       "  'kiss',\n",
       "  'carousel',\n",
       "  'shave',\n",
       "  'upward',\n",
       "  'banana',\n",
       "  'crab',\n",
       "  'snowboard',\n",
       "  'projector',\n",
       "  'downcast',\n",
       "  'pakistan',\n",
       "  'trolleybus',\n",
       "  'oil',\n",
       "  'toolbox',\n",
       "  'australia',\n",
       "  'libra',\n",
       "  'run',\n",
       "  'garlic',\n",
       "  'older',\n",
       "  'parrot',\n",
       "  'track',\n",
       "  'franc',\n",
       "  'smirk',\n",
       "  'china',\n",
       "  'mongolia',\n",
       "  'basket',\n",
       "  'person',\n",
       "  'claus:',\n",
       "  'stand',\n",
       "  'dancing:',\n",
       "  'bouvet',\n",
       "  'caribbean',\n",
       "  'bowing:',\n",
       "  'roll',\n",
       "  'letter',\n",
       "  'repeat',\n",
       "  'meat',\n",
       "  'note',\n",
       "  'water',\n",
       "  'fiji',\n",
       "  'cunha',\n",
       "  'eggplant',\n",
       "  'madagascar',\n",
       "  'sunris',\n",
       "  'lobster',\n",
       "  'bouquet',\n",
       "  'skunk',\n",
       "  'hedgehog',\n",
       "  'deliveri',\n",
       "  'remind',\n",
       "  'herzegovina',\n",
       "  'piec',\n",
       "  'eye',\n",
       "  'dizzi',\n",
       "  'headscarf:',\n",
       "  'dharma',\n",
       "  'grenadin',\n",
       "  'handshak',\n",
       "  'sierra',\n",
       "  'dress',\n",
       "  'marino',\n",
       "  'melilla',\n",
       "  'gesture:',\n",
       "  'shrimp',\n",
       "  'cream',\n",
       "  'tobago',\n",
       "  'zealand',\n",
       "  'om',\n",
       "  'tanzania',\n",
       "  'softbal',\n",
       "  'together:',\n",
       "  'last',\n",
       "  'bird',\n",
       "  '(keeling)',\n",
       "  'fairi',\n",
       "  'mount',\n",
       "  'butterfli',\n",
       "  'wheelchair',\n",
       "  'yang',\n",
       "  'egg',\n",
       "  'el',\n",
       "  'departur',\n",
       "  'vehicl',\n",
       "  'lantern',\n",
       "  'snail',\n",
       "  'neutral',\n",
       "  'kaaba',\n",
       "  'eleven',\n",
       "  'round',\n",
       "  'anguilla',\n",
       "  'boy:',\n",
       "  'facepalming:',\n",
       "  'unlock',\n",
       "  'rhinocero',\n",
       "  'milk',\n",
       "  'six-thirti',\n",
       "  'sailboat',\n",
       "  'cook:',\n",
       "  'bubbl',\n",
       "  'slash',\n",
       "  'book',\n",
       "  'loop',\n",
       "  'antenna',\n",
       "  'kitt',\n",
       "  'dark',\n",
       "  'foot',\n",
       "  'sahara',\n",
       "  'coupl',\n",
       "  'geni',\n",
       "  'peacock',\n",
       "  'palestinian',\n",
       "  'pictur',\n",
       "  'free',\n",
       "  'offic',\n",
       "  'timor-lest',\n",
       "  'firefight',\n",
       "  'armenia',\n",
       "  'heart-ey',\n",
       "  'wine',\n",
       "  'kneeling:',\n",
       "  'hot',\n",
       "  'blond',\n",
       "  'flash',\n",
       "  'japanes',\n",
       "  'link',\n",
       "  'aerial',\n",
       "  'hook',\n",
       "  'one-thirti',\n",
       "  'laugh',\n",
       "  'thumb',\n",
       "  'peac',\n",
       "  'ant',\n",
       "  'twelve-thirti',\n",
       "  'pickup',\n",
       "  'turk',\n",
       "  'estonia',\n",
       "  'briefcas',\n",
       "  'scale',\n",
       "  'fast-forward',\n",
       "  'bosnia',\n",
       "  'morocco',\n",
       "  'czechia',\n",
       "  'pin',\n",
       "  'swim',\n",
       "  'fear',\n",
       "  'scissor',\n",
       "  'gear',\n",
       "  'worker:',\n",
       "  'slovenia',\n",
       "  'bulgaria',\n",
       "  'pool',\n",
       "  'women',\n",
       "  'anguish',\n",
       "  'clown',\n",
       "  'play',\n",
       "  'zimbabw',\n",
       "  'montenegro',\n",
       "  'badminton',\n",
       "  '&',\n",
       "  'pretzel',\n",
       "  'lizard',\n",
       "  'kiss',\n",
       "  'safeti',\n",
       "  'flag:',\n",
       "  'ari',\n",
       "  'belarus',\n",
       "  'conveni',\n",
       "  '‚Äúmonth',\n",
       "  'eyebrow',\n",
       "  'cold',\n",
       "  'cl',\n",
       "  'ferri',\n",
       "  'eleven-thirti',\n",
       "  'nauru',\n",
       "  'tamal',\n",
       "  'up-down',\n",
       "  'woman',\n",
       "  'cracker',\n",
       "  'razor',\n",
       "  'kiwi',\n",
       "  'print',\n",
       "  'disguis',\n",
       "  'claus',\n",
       "  'wrench',\n",
       "  'beam',\n",
       "  'card',\n",
       "  'ribbon',\n",
       "  'fist',\n",
       "  'firefighter:',\n",
       "  'desktop',\n",
       "  'niger',\n",
       "  'eject',\n",
       "  'romania',\n",
       "  'left-right',\n",
       "  'guinea-bissau',\n",
       "  'microscop',\n",
       "  'wilt',\n",
       "  'bus',\n",
       "  'right',\n",
       "  'snow-cap',\n",
       "  'cactus',\n",
       "  'antarctica',\n",
       "  '‚Äúno',\n",
       "  'util',\n",
       "  'mahjong',\n",
       "  'jamaica',\n",
       "  'woman:',\n",
       "  'cityscap',\n",
       "  'sport',\n",
       "  'beer',\n",
       "  'diego',\n",
       "  'greec',\n",
       "  's√£o',\n",
       "  'thought',\n",
       "  'gem',\n",
       "  'tower',\n",
       "  'web',\n",
       "  'savor',\n",
       "  'cow',\n",
       "  'jordan',\n",
       "  'rico',\n",
       "  'paw',\n",
       "  'basketbal',\n",
       "  'cockroach',\n",
       "  'tire',\n",
       "  'papua',\n",
       "  'yawn',\n",
       "  'drum',\n",
       "  'charge‚Äù',\n",
       "  'fli',\n",
       "  'angola',\n",
       "  'backpack',\n",
       "  'shush',\n",
       "  'knife',\n",
       "  'disk',\n",
       "  'pirat',\n",
       "  'revolv',\n",
       "  'parachut',\n",
       "  'medium-light',\n",
       "  'magnifi',\n",
       "  'sock',\n",
       "  'flex',\n",
       "  'four',\n",
       "  'ng',\n",
       "  'comet',\n",
       "  'behind',\n",
       "  'funer',\n",
       "  'pencil',\n",
       "  'tanabata',\n",
       "  'sunglass',\n",
       "  'persev',\n",
       "  'railway',\n",
       "  'spade',\n",
       "  'blowfish',\n",
       "  'lotion',\n",
       "  'inform',\n",
       "  'chequer',\n",
       "  'equatori',\n",
       "  'garden',\n",
       "  'sweat',\n",
       "  'mage:',\n",
       "  'bicycl',\n",
       "  'exclam',\n",
       "  'fondu',\n",
       "  'litter',\n",
       "  'circus',\n",
       "  'british',\n",
       "  'teacher',\n",
       "  'tajikistan',\n",
       "  'high-spe',\n",
       "  'window',\n",
       "  'optic',\n",
       "  'european',\n",
       "  'bow',\n",
       "  'martial',\n",
       "  'circl',\n",
       "  'tunisia',\n",
       "  'zombi',\n",
       "  'hand:',\n",
       "  'puzzl',\n",
       "  'beach',\n",
       "  'prayer',\n",
       "  'america',\n",
       "  'banjo',\n",
       "  'hondura',\n",
       "  'plug',\n",
       "  'tom√©',\n",
       "  'libya',\n",
       "  'credit',\n",
       "  'vulcan',\n",
       "  'clipboard',\n",
       "  'kiribati',\n",
       "  'see-no-evil',\n",
       "  'church',\n",
       "  '‚Äúfree',\n",
       "  'dot',\n",
       "  'grade‚Äù',\n",
       "  'kazakhstan',\n",
       "  'close',\n",
       "  'bikini',\n",
       "  'fairy:',\n",
       "  'broom',\n",
       "  'wheelchair:',\n",
       "  'loudspeak',\n",
       "  'astronaut:',\n",
       "  'pierr',\n",
       "  'lower',\n",
       "  'pensiv',\n",
       "  'cyclon',\n",
       "  'lao',\n",
       "  'venezuela',\n",
       "  'seal',\n",
       "  'hungari',\n",
       "  'game',\n",
       "  'eleph',\n",
       "  'shop',\n",
       "  'board',\n",
       "  'hors',\n",
       "  'red',\n",
       "  'pick',\n",
       "  'biohazard',\n",
       "  'georgia',\n",
       "  'foggi',\n",
       "  'lemon',\n",
       "  'dish',\n",
       "  'flat',\n",
       "  'ship',\n",
       "  'bangladesh',\n",
       "  'rosett',\n",
       "  'bucket',\n",
       "  'sauropod',\n",
       "  'finger',\n",
       "  'neckti',\n",
       "  'djibouti',\n",
       "  'fuji',\n",
       "  'bow',\n",
       "  'out',\n",
       "  'receipt',\n",
       "  'floor',\n",
       "  'nerd',\n",
       "  'amount‚Äù',\n",
       "  'tokelau',\n",
       "  'northern',\n",
       "  'entri',\n",
       "  'monaco',\n",
       "  '‚Äúdiscount‚Äù',\n",
       "  'bandag',\n",
       "  'thermomet',\n",
       "  'western',\n",
       "  'technologist:',\n",
       "  'nut',\n",
       "  'bunni',\n",
       "  'farmer',\n",
       "  'aid',\n",
       "  'umbrella',\n",
       "  'firecrack',\n",
       "  'trophi',\n",
       "  'sri',\n",
       "  'paperclip',\n",
       "  'statu',\n",
       "  'cherri',\n",
       "  'broccoli',\n",
       "  'leg:',\n",
       "  'switzerland',\n",
       "  'iraq',\n",
       "  'malta',\n",
       "  'left',\n",
       "  'new',\n",
       "  'wrestl',\n",
       "  'lesotho',\n",
       "  'rain',\n",
       "  'frowning:',\n",
       "  'snake',\n",
       "  'trumpet',\n",
       "  'taxi',\n",
       "  'denmark',\n",
       "  'medium',\n",
       "  'high-heel',\n",
       "  'watch',\n",
       "  'ledger',\n",
       "  'mermaid:',\n",
       "  'mx',\n",
       "  'botswana',\n",
       "  'greenland',\n",
       "  'asia-australia',\n",
       "  'clapper',\n",
       "  'salad',\n",
       "  'scarf',\n",
       "  'speak',\n",
       "  'violin',\n",
       "  'massage:',\n",
       "  'up-right',\n",
       "  'kyrgyzstan',\n",
       "  'school',\n",
       "  'pan',\n",
       "  'pine',\n",
       "  'point',\n",
       "  '‚Äúcongratulations‚Äù',\n",
       "  'saxophon',\n",
       "  'net',\n",
       "  'slot',\n",
       "  'gabon',\n",
       "  'smile',\n",
       "  'mountain',\n",
       "  'level',\n",
       "  'up-left',\n",
       "  'bathtub',\n",
       "  'sleep',\n",
       "  'short',\n",
       "  'derelict',\n",
       "  'seven',\n",
       "  'fortun',\n",
       "  'book',\n",
       "  'ocean',\n",
       "  'light',\n",
       "  'hamburg',\n",
       "  'honey',\n",
       "  'eight-point',\n",
       "  '3rd',\n",
       "  'moon',\n",
       "  'pepper',\n",
       "  'wear',\n",
       "  'custom',\n",
       "  'netherland',\n",
       "  'glove',\n",
       "  'planet',\n",
       "  'baby:',\n",
       "  'bowl',\n",
       "  'diamond',\n",
       "  'vanuatu',\n",
       "  'babi',\n",
       "  'flatbread',\n",
       "  'c√¥te',\n",
       "  '(blood',\n",
       "  'men',\n",
       "  'tokyo',\n",
       "  'racing:',\n",
       "  'bag',\n",
       "  'keyboard',\n",
       "  'bicycl',\n",
       "  'david',\n",
       "  'tone',\n",
       "  'health',\n",
       "  'island',\n",
       "  'multipli',\n",
       "  'bar',\n",
       "  'candi',\n",
       "  'latin',\n",
       "  'elf:',\n",
       "  'mute',\n",
       "  'hourglass',\n",
       "  'central',\n",
       "  'outbox',\n",
       "  'asterisk',\n",
       "  'salute:',\n",
       "  'chick',\n",
       "  'hospit',\n",
       "  'key',\n",
       "  'tea',\n",
       "  'squid',\n",
       "  'satellit',\n",
       "  'horn',\n",
       "  'surfing:',\n",
       "  'bar',\n",
       "  'next',\n",
       "  'mountain',\n",
       "  'place',\n",
       "  'doubl',\n",
       "  'arab',\n",
       "  'rat',\n",
       "  'oden',\n",
       "  'shield',\n",
       "  'nicaragua',\n",
       "  'flower',\n",
       "  'grow',\n",
       "  'petri',\n",
       "  'macao',\n",
       "  'pinch',\n",
       "  'aquarius',\n",
       "  'purpl',\n",
       "  'tuvalu',\n",
       "  'bridg',\n",
       "  'surinam',\n",
       "  'ring',\n",
       "  'alien',\n",
       "  'mobil',\n",
       "  'heart',\n",
       "  'spain',\n",
       "  'ring',\n",
       "  'supervillain',\n",
       "  'india',\n",
       "  'haircut:',\n",
       "  'norfolk',\n",
       "  'pot',\n",
       "  '‚Äúhere‚Äù',\n",
       "  'dominican',\n",
       "  'latvia',\n",
       "  'azerbaijan',\n",
       "  'hike',\n",
       "  'lock',\n",
       "  'camel',\n",
       "  'worri',\n",
       "  'chess',\n",
       "  'sign',\n",
       "  'leg',\n",
       "  'fax',\n",
       "  '‚Äúnot',\n",
       "  'divid',\n",
       "  'up:',\n",
       "  'lab',\n",
       "  'wax',\n",
       "  'guiana',\n",
       "  '7',\n",
       "  'american',\n",
       "  'evergreen',\n",
       "  'two',\n",
       "  'yin',\n",
       "  'cano',\n",
       "  'link',\n",
       "  'left-fac',\n",
       "  't-shirt',\n",
       "  'poland',\n",
       "  'balloon',\n",
       "  'bahama',\n",
       "  'lotus',\n",
       "  'sheaf',\n",
       "  'chair',\n",
       "  'marshal',\n",
       "  'skullcap',\n",
       "  'printer',\n",
       "  'plus',\n",
       "  'arm',\n",
       "  'breast-feed',\n",
       "  'posit',\n",
       "  'type)',\n",
       "  'st.',\n",
       "  'depart',\n",
       "  'rais',\n",
       "  'seedl',\n",
       "  'shallow',\n",
       "  'saucer',\n",
       "  'magnet',\n",
       "  'slovakia',\n",
       "  'footbal',\n",
       "  'cayman',\n",
       "  'seat',\n",
       "  'aid:',\n",
       "  'syring',\n",
       "  'superhero:',\n",
       "  'camp',\n",
       "  'bulb',\n",
       "  'bicep',\n",
       "  'costa',\n",
       "  'pi√±ata',\n",
       "  'passeng',\n",
       "  'page',\n",
       "  'tilt',\n",
       "  'wave',\n",
       "  'sandwich',\n",
       "  'swirl',\n",
       "  'nine-thirti',\n",
       "  'cook',\n",
       "  'minus',\n",
       "  'hotel',\n",
       "  'clockwis',\n",
       "  'dusk',\n",
       "  'albania',\n",
       "  'explod',\n",
       "  'pout',\n",
       "  'takeout',\n",
       "  'volcano',\n",
       "  'kimono',\n",
       "  'moldova',\n",
       "  'malawi',\n",
       "  'shoe',\n",
       "  'boot',\n",
       "  'serbia',\n",
       "  'snowman',\n",
       "  'store',\n",
       "  'rica',\n",
       "  'speech',\n",
       "  'guinea',\n",
       "  'batteri',\n",
       "  'nevi',\n",
       "  'mayen',\n",
       "  'fri',\n",
       "  'rainbow',\n",
       "  'potato',\n",
       "  'mirror',\n",
       "  'liberti',\n",
       "  'cane:',\n",
       "  'memo',\n",
       "  'music',\n",
       "  'tulip',\n",
       "  'ticket',\n",
       "  'alarm',\n",
       "  'auto',\n",
       "  'cyprus',\n",
       "  'handbal',\n",
       "  'yen',\n",
       "  'panama',\n",
       "  'pile',\n",
       "  'sleepi',\n",
       "  'keycap:',\n",
       "  'scream',\n",
       "  'selfi',\n",
       "  'e-mail',\n",
       "  'cancer',\n",
       "  'record',\n",
       "  'tear',\n",
       "  'stone',\n",
       "  'snowboarder:',\n",
       "  'leaf',\n",
       "  'togo',\n",
       "  'chestnut',\n",
       "  'locomot',\n",
       "  'inbox',\n",
       "  'dot',\n",
       "  '‚Äúacceptable‚Äù',\n",
       "  'qatar',\n",
       "  'ophiuchus',\n",
       "  '*',\n",
       "  'bolt',\n",
       "  'princess',\n",
       "  'quarter',\n",
       "  'fuel',\n",
       "  'palau',\n",
       "  'solomon',\n",
       "  'femal',\n",
       "  'mechanic:',\n",
       "  'hatch',\n",
       "  'mushroom',\n",
       "  'fog',\n",
       "  'mermaid',\n",
       "  'pole',\n",
       "  'box',\n",
       "  'scientist:',\n",
       "  'abacus',\n",
       "  'seychell',\n",
       "  'mauritania',\n",
       "  'korea',\n",
       "  'lie',\n",
       "  'pineappl',\n",
       "  'guard',\n",
       "  'flashlight',\n",
       "  'dash',\n",
       "  'big',\n",
       "  'facepalm',\n",
       "  'croatia',\n",
       "  'antigua',\n",
       "  'down-right',\n",
       "  'namibia',\n",
       "  'carpentri',\n",
       "  'bagel',\n",
       "  'bookmark',\n",
       "  'futuna',\n",
       "  'maldiv',\n",
       "  'tangerin',\n",
       "  'polo:',\n",
       "  'mapl',\n",
       "  'cork',\n",
       "  '√•land',\n",
       "  'poo',\n",
       "  'fleur-de-li',\n",
       "  'republ',\n",
       "  'grimac',\n",
       "  'singapor',\n",
       "  'guyana',\n",
       "  'elf',\n",
       "  'star',\n",
       "  'lamp',\n",
       "  'coconut',\n",
       "  'pot',\n",
       "  'fri',\n",
       "  '1',\n",
       "  'magic',\n",
       "  'salt',\n",
       "  'sparkl',\n",
       "  'kinshasa',\n",
       "  'suspens',\n",
       "  'man',\n",
       "  'first',\n",
       "  'seneg',\n",
       "  'polish',\n",
       "  'droplet',\n",
       "  'wave',\n",
       "  'tip',\n",
       "  'drop',\n",
       "  'corn',\n",
       "  'voltag',\n",
       "  'firework',\n",
       "  'clipperton',\n",
       "  'warn',\n",
       "  'pitcairn',\n",
       "  'meridian',\n",
       "  'pilot',\n",
       "  'bath:',\n",
       "  'merperson',\n",
       "  'deaf',\n",
       "  'tent',\n",
       "  'vampire:',\n",
       "  'shrug',\n",
       "  'barbuda',\n",
       "  'ascens',\n",
       "  'headscarf',\n",
       "  'three',\n",
       "  'african',\n",
       "  'ski',\n",
       "  'bald',\n",
       "  'glass',\n",
       "  'bullet',\n",
       "  'phone',\n",
       "  'televis',\n",
       "  'expressionless',\n",
       "  'medium-smal',\n",
       "  'crown',\n",
       "  'gift',\n",
       "  'swan',\n",
       "  'biceps:',\n",
       "  'eighteen',\n",
       "  'burkina',\n",
       "  'pawn',\n",
       "  'rock',\n",
       "  'beverag',\n",
       "  'bounc',\n",
       "  'baguett',\n",
       "  'island',\n",
       "  'altern',\n",
       "  'da',\n",
       "  'drink',\n",
       "  'owl',\n",
       "  'peach',\n",
       "  'copyright',\n",
       "  'breast-feeding:',\n",
       "  'can',\n",
       "  'singer:',\n",
       "  'caledonia',\n",
       "  'fast',\n",
       "  'hippopotamus',\n",
       "  'robot',\n",
       "  'beginn',\n",
       "  'angel',\n",
       "  'gibbous',\n",
       "  'england',\n",
       "  'mug',\n",
       "  'sandwich',\n",
       "  'hollow',\n",
       "  'folder',\n",
       "  'montserrat',\n",
       "  'man',\n",
       "  'clamp',\n",
       "  'oman',\n",
       "  'paperclip',\n",
       "  'mode',\n",
       "  'white',\n",
       "  'running:',\n",
       "  'barber',\n",
       "  'non-pot',\n",
       "  'bat',\n",
       "  'bento',\n",
       "  'mask',\n",
       "  'face',\n",
       "  'letter',\n",
       "  'ruler',\n",
       "  'high',\n",
       "  'clover',\n",
       "  'clink',\n",
       "  'lanka',\n",
       "  'teapot',\n",
       "  'minibus',\n",
       "  'bermuda',\n",
       "  'cura√ßao',\n",
       "  'die',\n",
       "  'mailbox',\n",
       "  'long',\n",
       "  'megaphon',\n",
       "  'sint',\n",
       "  'club',\n",
       "  'benin',\n",
       "  'militari',\n",
       "  'videocassett',\n",
       "  'butter',\n",
       "  ...])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_score(val_dict, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 0.8036147356033325),\n",
       " ('3', 0.7338453531265259),\n",
       " ('4', 0.5989294052124023),\n",
       " ('8', 0.5632615089416504),\n",
       " ('13', 0.5582780838012695),\n",
       " ('6', 0.5528515577316284),\n",
       " ('10', 0.5463414788246155),\n",
       " ('12', 0.5445541739463806),\n",
       " ('per', 0.521578848361969),\n",
       " ('9', 0.5070649981498718),\n",
       " ('7', 0.500156044960022),\n",
       " ('48', 0.4976421296596527),\n",
       " ('800', 0.4964720606803894),\n",
       " ('5', 0.49304234981536865),\n",
       " ('36', 0.4899243712425232),\n",
       " ('4566', 0.4803587794303894),\n",
       " ('1‚Ä¶', 0.4803035259246826),\n",
       " ('11', 0.4702182412147522),\n",
       " ('üì£1,000', 0.46588796377182007),\n",
       " ('1/2', 0.4515560567378998)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('1',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = \"under\"\n",
    "[word for word in sentense.lower().split(\" \") if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pipeline(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
