{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "--------------\n",
    "## first check the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18866900 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many lines\n",
    "! wc -l data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209430000 data/emojitweets-01-04-2018.txt\n"
     ]
    }
   ],
   "source": [
    "# check how many words\n",
    "! wc -w data/emojitweets-01-04-2018.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.100392751326398"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On average the sentense are have 11 words\n",
    "209430000/18866900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test - validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15093520.0, 1886690.0, 3773380.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on 80-10-10\n",
    "18866900 * 0.8, 18866900 * 0.1, 18866900 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -15093520 data/emojitweets-01-04-2018.txt >>emoji_train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: error writing 'standard output': Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! (tail -3773380 data/emojitweets-01-04-2018.txt | head -1886690) >> emoji_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -1886690 data/emojitweets-01-04-2018.txt >> emoji_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate small_sample\n",
    "! head -1509352 data/emoji_train.txt >> data/emoji_ss.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec \n",
    "  \n",
    "# importing all necessary modules \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"emoji_1M.txt\", \"r\") \n",
    "sample = open(\"emoji_train.txt\", \"r\") \n",
    "s = sample.read() \n",
    "  \n",
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") \n",
    "\n",
    "\n",
    "\n",
    "# replace all \n",
    "signs = [\",\", \".\", \"(\" ,\")\", \"'\", '/\"', \":\", \";\", \"/\", \"(\", \")\", \"‚Äú\", \"‚Äù\", \"-\", \"+\", \"#\"]\n",
    "for sign in signs:\n",
    "    f = f.replace(sign, \"\")\n",
    "  \n",
    "data = [] \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to helper\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = open(\"../emoji_100k.txt\", \"r\") \n",
    "# s_ss = sample.read() \n",
    "  \n",
    "# # Replaces escape character with space\n",
    "# f_ss = s_ss.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "# # replace all \n",
    "# signs = [\",\", \".\", \"(\" ,\")\", \"'\", '/\"', \":\", \";\", \"/\", \"(\", \")\"]\n",
    "# for sign in signs:\n",
    "#     f_ss = f_ss.replace(sign, \"\")\n",
    "  \n",
    "# data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = f_ss\n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and snowball stemmer\n",
    "out = [[word for word in words if word not in stop] for words in data]\n",
    "snowball = SnowballStemmer('english')\n",
    "docs_snowball = [[snowball.stem(word) for word in words] for words in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad',\n",
       " 'arriv',\n",
       " 'game',\n",
       " '2',\n",
       " 'üöÄ',\n",
       " 'dude',\n",
       " 'like',\n",
       " '5',\n",
       " '‚Äô',\n",
       " '8',\n",
       " '140',\n",
       " 'pound',\n",
       " 'dick',\n",
       " 'long',\n",
       " 'strongalway',\n",
       " 'littl',\n",
       " 'dude',\n",
       " 'carri',\n",
       " 'üçÜ',\n",
       " '\\U0001f92aüôÉ',\n",
       " 'followersüëá',\n",
       " 'cant',\n",
       " 'breatiuhw',\n",
       " 'üíÄüíÄüíÄ',\n",
       " '2Ô∏è‚É£4Ô∏è‚É£',\n",
       " 'hour',\n",
       " 'til',\n",
       " 'schedul',\n",
       " 'drop',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_snowball[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save emoji list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emoji_list = list(emoji_set)\n",
    "# with open(\"emoji_list.csv\", 'w', newline='') as myfile:\n",
    "#      w = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#      w.writerow(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emoji_list.csv\", 'r', newline='') as myfile:\n",
    "     emoji_list = csv.reader(myfile, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(docs_snowball, min_count = 10,  \n",
    "                              size = 200, window = 5,\n",
    "                                workers=8 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "import pickle\n",
    "with open('cbow_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 19961500,\n",
       " 'vectors': 31938400,\n",
       " 'syn1neg': 31938400,\n",
       " 'total': 83838300}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.estimate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/emoji_val.2csv\",index_col=0)\n",
    "emoji_list = val_df[\"emoji\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to helper\n",
    "def emoji_prodictor(word, model):\n",
    "    sim = model.most_similar(word,topn=75)\n",
    "    for i in sim:\n",
    "        if i[0] in emoji_list:\n",
    "            return i[0]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'go', 'groceri', 'buy', 'appl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move to helper\n",
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out\n",
    "    \n",
    "    \n",
    "s1 = \"let's go grocery and buy some apple\"\n",
    "word_pipeline(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôÖüèΩ‚Äç‚ôÄÔ∏è\n",
      "üò¥\n",
      "üõç\n",
      "buy\n",
      "üéπ\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery and buy some apple\"\n",
    "for w in word_pipeline(s1):\n",
    "    print(emoji_prodictor(w, model1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üëåüèº'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor(\"today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skip-gram model \n",
    "model2 = gensim.models.Word2Vec(docs_snowball, \n",
    "                                min_count = 5,  \n",
    "                                size = 100, \n",
    "                                window = 5,\n",
    "                                sg = 1,\n",
    "                                workers=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save skip-gram model\n",
    "import pickle\n",
    "with open('sg_ss.pkl', 'wb') as f:\n",
    "    pickle.dump(model2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let\n",
      "go\n",
      "groceri\n",
      "buy\n",
      "appl\n"
     ]
    }
   ],
   "source": [
    "s1 = \"let's go grocery and buy some apple\"\n",
    "for w in word_pipeline(s1):\n",
    "    print(emoji_prodictor(w, model2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lt‚Äî‚Äî', 0.8018146753311157),\n",
       " ('recover‚Ä¶', 0.7987035512924194),\n",
       " ('wichita', 0.7942888140678406),\n",
       " ('hoeless', 0.7892517447471619),\n",
       " ('üëéüèºüëéüèº', 0.7889255285263062),\n",
       " ('philanthropi', 0.7855656147003174),\n",
       " ('concess', 0.783519446849823),\n",
       " ('earplug', 0.7825350165367126),\n",
       " ('¬£10k', 0.7814924716949463),\n",
       " ('conti', 0.7789944410324097),\n",
       " ('2k16', 0.7770678400993347),\n",
       " ('braai', 0.7757090926170349),\n",
       " ('üëç‚öΩÔ∏è', 0.7756443023681641),\n",
       " ('azpilicueta', 0.7747820615768433),\n",
       " ('‚ò™Ô∏è', 0.7731027007102966),\n",
       " ('üë©\\u200düíº', 0.7727632522583008),\n",
       " ('ravag', 0.772744357585907),\n",
       " ('person‚ù§Ô∏è', 0.7700779438018799),\n",
       " ('girls-', 0.7693312168121338),\n",
       " ('ü§îis', 0.7692635655403137)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar(\"am\",topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/emoji_val.2csv\",index_col=0)\n",
    "emoji_set = val_df[\"emoji\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"full_desc\"] = val_df.full_desc.apply(lambda x: x.strip('][').split(', '))\n",
    "val_df[\"desc2\"] = val_df.desc2.apply(lambda x: x.strip('][').split(', ') if isinstance(x,str) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in val_df.index:\n",
    "    for j in range(len(val_df.full_desc[i])):\n",
    "#         print(i)\n",
    "        val_df.full_desc[i][j] = val_df.full_desc[i][j].strip(\"/'\")\n",
    "    if val_df.desc2[i]:\n",
    "        for k in range(len(val_df.desc2[i])):\n",
    "    #         print(i)\n",
    "            val_df.desc2[i][k] = val_df.desc2[i][k].strip(\"/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grin'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"desc2\"][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation set\n",
    "val_dict = {}\n",
    "lst = []\n",
    "for i in val_df.index.tolist():\n",
    "    lst.extend(val_df.full_desc[i])\n",
    "    lst.extend(val_df.desc2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(set(lst))\n",
    "for val in vals:\n",
    "    val_dict[val] = []\n",
    "    for index in val_df.index:\n",
    "        \n",
    "        if val in val_df.full_desc[index]:\n",
    "            val_dict[val].append(val_df.emoji[index])\n",
    "        elif val in val_df.desc2[index]:\n",
    "            val_dict[val].append(val_df.emoji[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('val_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_dict.pickle', 'rb') as handle:\n",
    "    val_dict = pickle.load(handle)\n",
    "\n",
    "# print val_dict == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_score(val_dict, model):\n",
    "    total_words = 0\n",
    "    total_predicts = 0\n",
    "    correct = 0\n",
    "    words = []\n",
    "    word_vectors = model.wv\n",
    "    for word in val_dict:\n",
    "#         print(word)\n",
    "        if word_pipeline(word):\n",
    "            w = word_pipeline(word)[0]\n",
    "            words.append(w)\n",
    "            if w in word_vectors:\n",
    "                total_words += 1\n",
    "                if emoji_prodictor(w, model) in val_dict[word]:\n",
    "                    correct += 1\n",
    "                    total_predicts += 1\n",
    "                elif emoji_prodictor(w, model) in emoji_set:\n",
    "                    total_predicts += 1\n",
    "    return correct/total_words, correct/total_predicts, words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = word2vec_score(val_dict, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = word2vec_score(val_dict, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.14964788732394366, 0.18695014662756598),\n",
       " (0.15307262569832403, 0.23539518900343642))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score for CBOW and skipgram for small sample with SnowballStemmer\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score for CBOW and skipgram for full train sample with SnowballStemmer\n",
    "score1[0:2],score2[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'stethoscop',\n",
       " 'yoga',\n",
       " 'bellhop',\n",
       " 'new',\n",
       " 'fax',\n",
       " 'red',\n",
       " 'decor',\n",
       " 'manicur',\n",
       " 'rais',\n",
       " 'beaver',\n",
       " 'ballet',\n",
       " 'guyana',\n",
       " 'health',\n",
       " 'faro',\n",
       " 'footprint',\n",
       " 'suspens',\n",
       " 'superhero:',\n",
       " 'zambia',\n",
       " 'punch',\n",
       " 'pill',\n",
       " 'ok',\n",
       " 'industri',\n",
       " 'ice',\n",
       " 'cuba',\n",
       " 'curl',\n",
       " 'die',\n",
       " 'pancak',\n",
       " 'mantilla',\n",
       " 'milk',\n",
       " 'skunk',\n",
       " 'shop',\n",
       " 'clean',\n",
       " 'motorway',\n",
       " 'backhand',\n",
       " 'netherland',\n",
       " 'quarter',\n",
       " 'bermuda',\n",
       " 'sew',\n",
       " 'outlin',\n",
       " 'wale',\n",
       " 'hook',\n",
       " 'state',\n",
       " 'peopl',\n",
       " 'polic',\n",
       " 'island',\n",
       " 'aquarius',\n",
       " 'potabl',\n",
       " 'cook',\n",
       " \"o'clock\",\n",
       " 'gua',\n",
       " 'two-hump',\n",
       " 'nevi',\n",
       " 'monkey',\n",
       " 'christma',\n",
       " 'young',\n",
       " 'santa',\n",
       " 'butterfli',\n",
       " 'revolv',\n",
       " 'mate',\n",
       " 'emblem',\n",
       " 'antarctica',\n",
       " 'guardsman',\n",
       " 'wrestl',\n",
       " 'ferri',\n",
       " 'cat',\n",
       " 'england',\n",
       " 'blowfish',\n",
       " 'cross',\n",
       " 'person',\n",
       " 'luxembourg',\n",
       " 'open',\n",
       " 'comput',\n",
       " 'unspecifi',\n",
       " 'tab',\n",
       " 'away',\n",
       " 'eight-spok',\n",
       " 'babi',\n",
       " 'blond',\n",
       " 'curri',\n",
       " 'floppi',\n",
       " 'ribbon',\n",
       " 'samoa',\n",
       " 'knife',\n",
       " 'fantasi',\n",
       " 'barth√©lemi',\n",
       " 'race',\n",
       " 'femal',\n",
       " 'man',\n",
       " 'cream',\n",
       " 'stone',\n",
       " 'bulgaria',\n",
       " 'boat:',\n",
       " 'comic',\n",
       " 'board',\n",
       " 'santa',\n",
       " 'detect',\n",
       " 'fork',\n",
       " 'vampir',\n",
       " 'witch',\n",
       " 'b',\n",
       " 'stopwatch',\n",
       " 'chopstick',\n",
       " 'elev',\n",
       " 'territori',\n",
       " 'person',\n",
       " 'triangl',\n",
       " 'lao',\n",
       " 'capricorn',\n",
       " 'bicycl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1[2][::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üì©üíµ', 0.7020508050918579),\n",
       " ('üí∞üíØ', 0.6812326312065125),\n",
       " ('ü§∑üèª\\u200d‚ôÄÔ∏èüí∞', 0.6755300760269165),\n",
       " ('üî•get', 0.6755296587944031),\n",
       " ('ü§ëüíµ', 0.6684743165969849),\n",
       " ('cash', 0.6657474040985107),\n",
       " ('ambaniüòÇüòÇ', 0.6505308747291565),\n",
       " ('tireddddd', 0.644366443157196),\n",
       " ('launder', 0.6404151320457458),\n",
       " ('ü§ëüí∞', 0.6393123269081116),\n",
       " ('rake', 0.6330356001853943),\n",
       " ('splurg', 0.6325438022613525),\n",
       " ('stack', 0.6323885321617126),\n",
       " ('ü§¶üèæ\\u200d‚ôÇÔ∏èü§£ü§£ü§£ü§£ü§£', 0.6310147643089294),\n",
       " ('flow‚ú®', 0.628703773021698),\n",
       " ('semperfi', 0.628624439239502),\n",
       " ('üí≥üíµ', 0.6212002038955688),\n",
       " ('ü§£ü§∑üèæ\\u200d‚ôÄÔ∏è', 0.620684802532196),\n",
       " ('taxpay', 0.6157459020614624),\n",
       " ('fiat', 0.615238606929779),\n",
       " ('üíµüíµüíµ', 0.6151927709579468),\n",
       " ('becu', 0.6137044429779053),\n",
       " ('heist', 0.6131662130355835),\n",
       " ('üá∫üá∏-armi', 0.612061083316803),\n",
       " ('bustedüëä', 0.611751139163971),\n",
       " ('basi', 0.610927939414978),\n",
       " ('shitüòíüíØ', 0.608547568321228),\n",
       " ('ü§¶üèæ\\u200d‚ôÄÔ∏èüôÑ', 0.6056149005889893),\n",
       " ('armani', 0.6028887629508972),\n",
       " ('üíÅüèº\\u200d‚ôÄÔ∏èüòÇ', 0.6028388738632202),\n",
       " ('fucküò≠', 0.6024332046508789),\n",
       " ('swisher', 0.601882815361023),\n",
       " ('twinstar', 0.6017516255378723),\n",
       " ('wanaa', 0.5995397567749023),\n",
       " ('¬£75', 0.5975872278213501),\n",
       " ('üí∞', 0.5971856117248535),\n",
       " ('debt', 0.5964198112487793),\n",
       " ('üí∞to', 0.5939447283744812),\n",
       " ('usaa', 0.5925270318984985),\n",
       " ('yansh', 0.5922828912734985),\n",
       " ('üíìüíò', 0.5900037884712219),\n",
       " ('reimburs', 0.5893838405609131),\n",
       " ('rack', 0.5884997248649597),\n",
       " ('üôãüèº\\u200d‚ôÄÔ∏è‚Ä¶', 0.5879998803138733),\n",
       " ('¬£‚Ä¶', 0.5870566964149475),\n",
       " ('buyin', 0.5869550704956055),\n",
       " ('komot', 0.5859046578407288),\n",
       " ('explin', 0.5855370163917542),\n",
       " ('trev', 0.5852842330932617),\n",
       " ('hassl', 0.5851836800575256)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.most_similar('money',topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pipeline(sentense):\n",
    "    out = [word for word in sentense.lower().split(\" \") if word not in stop]\n",
    "    out = [snowball.stem(word) for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense = \"under\"\n",
    "[word for word in sentense.lower().split(\" \") if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pipeline(sentense)model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòÄ'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor('us', model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üöå'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_prodictor('nyc', model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sherlock_homes.txt', 'r') as file:\n",
    "    text = file.read().lower()\n",
    "print('text length', len(text))\n",
    "chars = sorted(list(set(text))) # getting all unique chars\n",
    "print('total chars: ', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)\n",
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(500, 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
